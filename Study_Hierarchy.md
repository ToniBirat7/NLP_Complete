## **Hierarchy of Reading the Notes**

When studying the notes in this repository, it is beneficial to follow a structured hierarchy to maximize understanding and retention. Below is a suggested hierarchy for reading the notes:

### **First**

**Complete `PreRequisite` Folder**

1. `NLP_Basics.pdf` - Understand the fundamental concepts of Natural Language Processing.

2. `RNN_Notes.pdf` - Learn about Recurrent Neural Networks, their architecture, and applications.

3. `More_on_Word_Embeddings.pdf` - Dive deeper into word embeddings and their significance in NLP.

### **Second**

**Complete `Transformer` Folder**

1. `Self_Attention.pdf` - Grasp the concept of self-attention mechanisms in transformer models.

2. `MultiHeaded_Attention.pdf` - Study the multi-headed attention mechanism and its role in transformers.
