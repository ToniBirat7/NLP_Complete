{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c57237",
   "metadata": {},
   "source": [
    "## **Multi Head Attention**\n",
    "\n",
    "**Recap**\n",
    "\n",
    "First we had `Word Embeddings` which is static in nature i.e. each word has a fixed representation regardless of its context. We need a mechanism that can dynamically adjust these representations based on the surrounding words in a sentence. This is where `Self-Attention` comes into play.\n",
    "\n",
    "`Self-Attention` allows each word to attend to all other words in the sequence, enabling the model to capture contextual relationships effectively. With `Q` (`Query`), `K` (`Key`), and `V` (`Value`) vectors, we were able to compute attention scores and then generate context-aware representations for each word. So `Self-Attention` genereates `Contextual Embeddings` from `Word Embeddings`.\n",
    "\n",
    "**Promblem with Single Head Attention or Self-Attention**\n",
    "\n",
    "`Self-Attention` mechanism, was able to capture contextual relationships effectively. For example, if we've two sentences like `\"Money Bank Grows\"` and `\"River Bank Flows\"`, the word `\"Bank\"` would attend more to `\"Money\"` in the first sentence and more to `\"River\"` in the second sentence. This dynamic adjustment of attention based on context is a key strength of the `Self-Attention` mechanism.\n",
    "\n",
    "But what if we've a sentence like `\"The man saw the astronomer with the telescope\"`? Here, the word `\"with\"` can relate to either `\"astronomer\"` or `\"telescope\"`, leading to ambiguity. A single attention head might struggle to capture both interpretations effectively.\n",
    "\n",
    "This sentence can have two different interpretations:\n",
    "\n",
    "1. The man used the telescope to see the astronomer.\n",
    "\n",
    "2. The astronomer has the telescope.\n",
    "\n",
    "If we use `Self-Attention` with a single head, it might focus on `one interpretation` and miss the other. This is where `Multi-Head Attention` comes into play.\n",
    "\n",
    "To get the actual meaning of the sentence, we need to consider both interpretations simultaneously. `Multi-Head Attention` allows the model to do just that by having multiple attention heads, each focusing on different parts of the sentence.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In `Multi-Head Attention`, instead of having a single set of `Q`, `K`, and `V` matrices, we have multiple sets, each corresponding to a different attention head. Each head can learn to focus on different aspects of the input sequence.\n",
    "\n",
    "So if we've `h` attention heads, we have `h` different sets of `Q`, `K`, and `V` matrices. Each head computes its own attention scores and generates its own context-aware representations.\n",
    "\n",
    "### **Understanding `Multi-Head Attention` With Example**\n",
    "\n",
    "Let's say we've an input sentence: `\"Money Bank Grows\"`.\n",
    "\n",
    "Now for this sentence, we'll have multiple attention heads, say `h = 2` heads for simplicity.\n",
    "\n",
    "1. **Word Embeddings**: First, we convert each word into its corresponding `Word Embedding` or `Token Embedding`.\n",
    "\n",
    "$$\n",
    "E_{Money} = [0.1, 0.2, 0.3]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{Bank} = [0.4, 0.5, 0.6]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{Grows} = [0.7, 0.8, 0.9]\n",
    "$$\n",
    "\n",
    "2. **Weight Matrices for Each Head**: For each attention head, we have separate weight matrices for `Query`, `Key`, and `Value`. Let's denote them as `W_Q^1`, `W_K^1`, `W_V^1` for the first head or `First Set` and `W_Q^2`, `W_K^2`, `W_V^2` for the second head or `Second Set`.\n",
    "\n",
    "**First Head Weight Matrices**:\n",
    "\n",
    "$$\n",
    "W_Q^1 = \\begin{bmatrix}\n",
    "0.1 & 0.0 & 0.2 \\\\\n",
    "0.0 & 0.1 & 0.1 \\\\\n",
    "0.2 & 0.1 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_K^1 = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 \\\\\n",
    "0.1 & 0.2 & 0.1 \\\\\n",
    "0.0 & 0.1 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_V^1 = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.1 \\\\\n",
    "0.2 & 0.1 & 0.2 \\\\\n",
    "0.1 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Second Head Weight Matrices**:\n",
    "\n",
    "$$\n",
    "W_Q^2 = \\begin{bmatrix}\n",
    "0.0 & 0.2 & 0.1 \\\\\n",
    "0.1 & 0.0 & 0.2 \\\\\n",
    "0.2 & 0.1 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_K^2 = \\begin{bmatrix}\n",
    "0.1 & 0.0 & 0.2 \\\\\n",
    "0.2 & 0.1 & 0.0 \\\\\n",
    "0.0 & 0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_V^2 = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.2 \\\\\n",
    "0.1 & 0.2 & 0.1 \\\\\n",
    "0.1 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. **Computing Q, K, V for Each Head**: For each head, we compute the `Query`, `Key`, and `Value` vectors for each word by multiplying the `Word Embeddings` with the respective `Weight Matrices`.\n",
    "\n",
    "**First Head Computations**:\n",
    "\n",
    "For the word `\"Money\"` with embedding `E_{Money} = [0.1, 0.2, 0.3]`:\n",
    "\n",
    "$$\n",
    "Q_{Money}^1 = E_{Money} \\cdot W_Q^1 = [0.1, 0.2, 0.3] \\cdot \\begin{bmatrix}\n",
    "0.1 & 0.0 & 0.2 \\\\\n",
    "0.0 & 0.1 & 0.1 \\\\\n",
    "0.2 & 0.1 & 0.0\n",
    "\\end{bmatrix} = [0.07, 0.05, 0.04]\n",
    "$$\n",
    "\n",
    "$$\n",
    "K_{Money}^1 = E_{Money} \\cdot W_K^1 = [0.1, 0.2, 0.3] \\cdot \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 \\\\\n",
    "0.1 & 0.2 & 0.1 \\\\\n",
    "0.0 & 0.1 & 0.2\n",
    "\\end{bmatrix} = [0.04, 0.08, 0.07]\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{Money}^1 = E_{Money} \\cdot W_V^1 = [0.1, 0.2, 0.3] \\cdot \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.1 \\\\\n",
    "0.2 & 0.1 & 0.2 \\\\\n",
    "0.1 & 0.1 & 0.1\n",
    "\\end{bmatrix} = [0.08, 0.07, 0.09]\n",
    "$$\n",
    "\n",
    "Similarly, we compute for `\"Bank\"` and `\"Grows\"` for the first head.\n",
    "\n",
    "Then, we repeat the same process for the second head using `W_Q^2`, `W_K^2`, and `W_V^2`.\n",
    "\n",
    "4. **Attention Calculation for Each Head**: Each head computes its own attention scores and generates context-aware representations using the `Scaled Dot-Product Attention` mechanism.\n",
    "\n",
    "**First Head Attention**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}^1(Q^1, K^1, V^1) = \\text{Softmax}\\left(\\frac{Q^1 {K^1}^T}{\\sqrt{d_k}}\\right) V^1\n",
    "$$\n",
    "\n",
    "Where `d_k` is the dimension of the `Key` vectors.\n",
    "\n",
    "Here, we compute the dot product of the `Query` and `Key` vectors, scale it by dividing by the square root of `d_k`, apply the `Softmax` function to get the attention weights, and then multiply by the `Value` vectors to get the output for the first head.\n",
    "\n",
    "This will be the `First Contextual Embedding` for the sentence `\"Money Bank Grows\"` from the `First Head`.\n",
    "\n",
    "Which will look something like this:\n",
    "\n",
    "$$\n",
    "C^1 = \\begin{bmatrix}\n",
    "c_{Money}^1 \\\\\n",
    "c_{Bank}^1 \\\\\n",
    "c_{Grows}^1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Second Head Attention**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}^2(Q^2, K^2, V^2) = \\text{Softmax}\\left(\\frac{Q^2 {K^2}^T}{\\sqrt{d_k}}\\right) V^2\n",
    "$$\n",
    "\n",
    "Where `d_k` is the dimension of the `Key` vectors.\n",
    "\n",
    "Here, we compute the dot product of the `Query` and `Key` vectors, scale it by dividing by the square root of `d_k`, apply the `Softmax` function to get the attention weights, and then multiply by the `Value` vectors to get the output for the second head.\n",
    "\n",
    "This will be the `Second Contextual Embedding` for the sentence `\"Money Bank Grows\"` from the `Second Head`.\n",
    "\n",
    "Which will look something like this:\n",
    "\n",
    "$$\n",
    "C^2 = \\begin{bmatrix}\n",
    "c_{Money}^2 \\\\\n",
    "c_{Bank}^2 \\\\\n",
    "c_{Grows}^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we've two different context-aware representations for the same input sentence, each capturing different aspects of the relationships between the words. Previously, with single head attention, we had only one such representation.\n",
    "\n",
    "5. **Concatenation and Final Linear Transformation**: Finally, we concatenate the outputs from all attention heads and pass them through a final linear transformation to get the final output of the `Multi-Head Attention` mechanism.\n",
    "\n",
    "$$\n",
    "C = \\text{Concat}(C^1, C^2)\n",
    "$$\n",
    "\n",
    "We multiply this concatenated matrix with another weight matrix `W_O` i.e. `Apply Linear Transformation` to get the final output embeddings.\n",
    "\n",
    "$$\n",
    "O = C \\cdot W_O\n",
    "$$\n",
    "\n",
    "Where `W_O` is the weight matrix for the output linear transformation.\n",
    "\n",
    "`W_O` could look something like this:\n",
    "\n",
    "$$\n",
    "W_O = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.1 & 0.0 & 0.1 & 0.2 \\\\\n",
    "0.2 & 0.1 & 0.0 & 0.1 & 0.2 & 0.1 \\\\\n",
    "0.1 & 0.0 & 0.2 & 0.2 & 0.1 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "During `Backpropagation`, all these weight matrices (`W_Q^i`, `W_K^i`, `W_V^i`, and `W_O`) are updated to minimize the loss function, allowing the model to learn optimal attention patterns for different contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09cb29",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975188fa",
   "metadata": {},
   "source": [
    "## **How Does `Multi-Head Attention` Actually Works?**\n",
    "\n",
    "Below image explains the entire process of `Multi-Head Attention` step by step.\n",
    "\n",
    "<img src=\"../../../Notes_Images/Multi_Head_Attn.png\" alt=\"Multi Head Attention\" style=\"width:1200px;\"/>\n",
    "\n",
    "Then,\n",
    "\n",
    "<img src=\"../../../Notes_Images/Multi_Head_Attn2.png\" alt=\"Multi Head Attention Steps\" style=\"width:1200px;\"/>\n",
    "\n",
    "The output `Z` obtained after Mutliplying `Concat(head_1, head_2, ..., head_h)` with `W_O` is the final output of the `Multi-Head Attention` mechanism.\n",
    "\n",
    "This `Z` captures information from different representation subspaces at different positions, allowing the model to attend to various aspects of the input sequence simultaneously.\n",
    "\n",
    "**Why `Linear Transformations` i.e. `W_O` is required after Concatenation?**\n",
    "\n",
    "- These weights tries to create a balance between the different heads.\n",
    "\n",
    "- To get the size of the output same as input embedding size.\n",
    "\n",
    "### **Attention is All You Need**\n",
    "\n",
    "In the seminal paper \"Attention is All You Need\" by Vaswani et al., following were the sizes and dimensions used for `Multi-Head Attention`:\n",
    "\n",
    "**Embedding Dimension (d_model)**: 512\n",
    "\n",
    "**Number of Heads (h)**: 8\n",
    "\n",
    "**Dimension of Each Weight Matrix (W_Q, W_K, W_V)**: 64 (i.e., d_model/h = 512/8). So its shape will be (512, 64).\n",
    "\n",
    "- As the `Input Embedding Dimension` will have size of `(n, 512)` where `n` is the number of tokens in the input sequence.\n",
    "\n",
    "- And the `Weight Matrices` will have size of `(512, 64)`. So matrix multiplication will be possible. And the output `Q`, `K`, `V` will have size of `(n, 64)` for each head.\n",
    "\n",
    "- After computing attention for each head, we will have `8` different `Contextual Embeddings` for each token, each of size `(n, 64)`.\n",
    "\n",
    "- Then, we concatenate these `8` embeddings along the last dimension to get a combined embedding of size `(n, 512)`.\n",
    "\n",
    "- Then, this concatenated embedding is multiplied with the output weight matrix `W_O` of size `(512, 512)` to get the final output of the `Multi-Head Attention` mechanism, which also has size `(n, 512)`.\n",
    "\n",
    "This the `Input Embedding Dimension` and `Output Embedding Dimension` remains the same i.e. `512`, while allowing the model to attend to information from different representation subspaces through multiple heads.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
