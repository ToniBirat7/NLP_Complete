{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a96fa1",
   "metadata": {},
   "source": [
    "Till now we've understood `Self-Attention` and `Multi-Head Attention` mechanisms in detail.\n",
    "\n",
    "The major advantage of these mechanisms are:\n",
    "\n",
    "- Parellelization: Unlike `RNNs` which process sequences sequentially, `Self-Attention` allows for parallel processing of all tokens in a sequence, significantly speeding up training times.\n",
    "\n",
    "- Long-Range Dependencies: `Self-Attention` can capture dependencies between tokens regardless of their distance in the sequence, making it effective for understanding context over long texts. Basically, generates `Contextual Embeddings` for each word. So it considers the entire sentence while generating the embedding for a particular word.\n",
    "\n",
    "- Flexibility: `Self-Attention` can be applied to various types of data, including text, images, and more, making it a versatile tool in deep learning.\n",
    "\n",
    "**But,**\n",
    "\n",
    "This mechanism has a major `Drawback` i.e. **`Lack of Positional Information`**.\n",
    "\n",
    "Let's understand this with an example.\n",
    "\n",
    "Consider the sentences:\n",
    "\n",
    "S1 = `\"Lion Kills Deer\"`\n",
    "\n",
    "S2 = `\"Deer Kills Lion\"`\n",
    "\n",
    "If we calculate the `Self-Attention` for both sentences, the output `Contextual Embeddings` for the words `\"Lion\"` and `\"Deer\"` would be identical in both sentences, as `Self-Attention` does not take into account the order of words in the sequence. This means that the model would not be able to distinguish between the two sentences based on their meaning, as the positional information is lost.\n",
    "\n",
    "This should not happen as the meaning of both sentences is completely different. But due to the lack of positional information in `Self-Attention`, the model fails to capture this difference.\n",
    "\n",
    "Therefore, we can conclude that if we've a `Sentence` where the order of words matters, then `Self-Attention` alone is not sufficient to capture the meaning of the sentence.\n",
    "\n",
    "So we need a way to represent or preserve the order of words in a sequence while using `Self-Attention`.\n",
    "\n",
    "This is where **`Positional Encoding`** comes into play.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e3728",
   "metadata": {},
   "source": [
    "## **Positional Encoding**\n",
    "\n",
    "In `RNNs` and `LSTMs`, the order of words is inherently preserved due to their sequential processing nature. However, in `Self-Attention`, since all words are processed simultaneously, we need to explicitly provide information about the position of each word in the sequence.\n",
    "\n",
    "**`Positional Encoding`** is a technique used to inject information about the position of words in a sequence into the model. This is typically done by adding a positional encoding vector to each word embedding before feeding it into the `Self-Attention` mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fea3e9",
   "metadata": {},
   "source": [
    "## **Understanding `Positional Encoding` From `First Principles`**\n",
    "\n",
    "The very basic solution to pass the `Postional Information` to the `Self-Attention` mechanism is by adding an `Extra Dimension` to the `Word Embeddings` that represents the position of each word in the sequence.\n",
    "\n",
    "For example, consider the sentence: `\"Lion Kills Deer\"`.\n",
    "\n",
    "The `Word Embeddings` for the words `\"Lion\"`, `\"Kills\"`, and `\"Deer\"` can be represented as follows (assuming 3D embeddings for simplicity):\n",
    "\n",
    "$$\n",
    "\\text{Lion} = \\begin{bmatrix} 0.2 \\\\ 0.8 \\\\ 0.5 \\end{bmatrix}, \\quad\n",
    "\\text{Kills} = \\begin{bmatrix} 0.6 \\\\ 0.1 \\\\ 0.3 \\end{bmatrix}, \\quad\n",
    "\\text{Deer} = \\begin{bmatrix} 0.9 \\\\ 0.4 \\\\ 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we can add an `Extra Dimension` to each embedding to represent the position of each word in the sequence:\n",
    "\n",
    "$$\n",
    "\\text{Lion} = \\begin{bmatrix} 0.2 \\\\ 0.8 \\\\ 0.5 \\\\ 1 \\end{bmatrix}, \\quad\n",
    "\\text{Kills} = \\begin{bmatrix} 0.6 \\\\ 0.1 \\\\ 0.3 \\\\ 2 \\end{bmatrix}, \\quad\n",
    "\\text{Deer} = \\begin{bmatrix} 0.9 \\\\ 0.4 \\\\ 0.7 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, the last element in each embedding vector represents the position of the word in the sequence (1 for `\"Lion\"`, 2 for `\"Kills\"`, and 3 for `\"Deer\"`).\n",
    "\n",
    "**But, this approach has some limitations:**\n",
    "\n",
    "1. **Unbounded Values**: The positional values can grow indefinitely as the sequence length increases. This means that for very long sequences, the positional values can become very large, which may not be ideal for the model to learn effectively.\n",
    "\n",
    "2. **Discrete Values**: The positional values are discrete integers, which may not provide enough granularity for the model to learn subtle differences in word positions. The positional values can become very large for long sequences, which may lead to numerical instability during training.\n",
    "\n",
    "We know that `Neural Networks` work better with smaller values within the range of `-1 to 1` or `0 to 1` or `Continuous values`. So, if we've a sentence with `1000` words, the positional value for the last word would be `1000`, and during `Backpropagation`, this large value can lead to `Exploding Gradients`, making the training process unstable.\n",
    "\n",
    "3. **Can't Capture Relative Positions**: This method only captures absolute positions of words, not their relative positions. For example, in the sentences `\"The cat sat on the mat\"` and `\"The mat sat on the cat\"`, the relative positions of `\"cat\"` and `\"mat\"` are important for understanding the meaning, but this approach does not capture that information.\n",
    "\n",
    "Even if we think of normalizing these positional values by dividing them by the maximum length of the sequence, it still doesn't solve the problem of scalability and numerical instability completely.\n",
    "\n",
    "- Because, in practice, sequences can vary greatly in length, and normalizing by the maximum length may not be effective for all sequences. For sentences with different lengths, there will be no consistent way to represent positions, leading to potential confusion for the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "So, now we need a function that:\n",
    "\n",
    "- `Bounds` the positional values within a certain range (e.g., `-1 to 1` or `0 to 1`).\n",
    "\n",
    "- Provides `Continuous Values` for better learning.\n",
    "\n",
    "- Can `Capture Relative Positions` of words in addition to absolute positions i.e. `Periodic Function`.\n",
    "\n",
    "One such function that satisfies all these criteria is the **`Sine`** and **`Cosine`** functions.\n",
    "\n",
    "### **`Sine` Function for Positional Encoding**\n",
    "\n",
    "As `Sine` function oscillates between `-1` and `1`, it provides bounded values.\n",
    "\n",
    "Additionally, it is a continuous function, because for every `x` value, there exists a corresponding `y` value.\n",
    "\n",
    "And,\n",
    "\n",
    "it is periodic in nature, which means it can capture relative positions effectively.\n",
    "\n",
    "Below is the graph of `Sine` function:\n",
    "\n",
    "<img src=\"../../Notes_Images/Sine.png\" alt=\"Sine Function\" width=\"1200\"/>\n",
    "\n",
    "**How to use `Sine` function for `Positional Encoding`?**\n",
    "\n",
    "What we can do is, for each position `pos` in the sequence, we can compute the positional encoding using the `Sine` function as follows:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `PE(pos, 2i)` is the positional encoding for the even dimensions.\n",
    "\n",
    "- `PE(pos, 2i+1)` is the positional encoding for the odd dimensions.\n",
    "\n",
    "- `pos` is the position of the word in the sequence (0-indexed).\n",
    "\n",
    "- `i` is the dimension index.\n",
    "\n",
    "- `d_model` is the total dimension of the model (i.e., the size of the word embeddings).\n",
    "\n",
    "This way, we can generate positional encodings for each word in the sequence and add them to the corresponding word embeddings before feeding them into the `Self-Attention` mechanism.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
