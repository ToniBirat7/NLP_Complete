{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a96fa1",
   "metadata": {},
   "source": [
    "Till now we've understood `Self-Attention` and `Multi-Head Attention` mechanisms in detail.\n",
    "\n",
    "The major advantage of these mechanisms are:\n",
    "\n",
    "- Parellelization: Unlike `RNNs` which process sequences sequentially, `Self-Attention` allows for parallel processing of all tokens in a sequence, significantly speeding up training times.\n",
    "\n",
    "- Long-Range Dependencies: `Self-Attention` can capture dependencies between tokens regardless of their distance in the sequence, making it effective for understanding context over long texts. Basically, generates `Contextual Embeddings` for each word. So it considers the entire sentence while generating the embedding for a particular word.\n",
    "\n",
    "- Flexibility: `Self-Attention` can be applied to various types of data, including text, images, and more, making it a versatile tool in deep learning.\n",
    "\n",
    "**But,**\n",
    "\n",
    "This mechanism has a major `Drawback` i.e. **`Lack of Positional Information`**.\n",
    "\n",
    "Let's understand this with an example.\n",
    "\n",
    "Consider the sentences:\n",
    "\n",
    "S1 = `\"Lion Kills Deer\"`\n",
    "\n",
    "S2 = `\"Deer Kills Lion\"`\n",
    "\n",
    "If we calculate the `Self-Attention` for both sentences, the output `Contextual Embeddings` for the words `\"Lion\"` and `\"Deer\"` would be identical in both sentences, as `Self-Attention` does not take into account the order of words in the sequence. This means that the model would not be able to distinguish between the two sentences based on their meaning, as the positional information is lost.\n",
    "\n",
    "This should not happen as the meaning of both sentences is completely different. But due to the lack of positional information in `Self-Attention`, the model fails to capture this difference.\n",
    "\n",
    "Therefore, we can conclude that if we've a `Sentence` where the order of words matters, then `Self-Attention` alone is not sufficient to capture the meaning of the sentence.\n",
    "\n",
    "So we need a way to represent or preserve the order of words in a sequence while using `Self-Attention`.\n",
    "\n",
    "This is where **`Positional Encoding`** comes into play.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e3728",
   "metadata": {},
   "source": [
    "## **Positional Encoding**\n",
    "\n",
    "In `RNNs` and `LSTMs`, the order of words is inherently preserved due to their sequential processing nature. However, in `Self-Attention`, since all words are processed simultaneously, we need to explicitly provide information about the position of each word in the sequence.\n",
    "\n",
    "**`Positional Encoding`** is a technique used to inject information about the position of words in a sequence into the model. This is typically done by adding a positional encoding vector to each word embedding before feeding it into the `Self-Attention` mechanism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fea3e9",
   "metadata": {},
   "source": [
    "## **Understanding `Positional Encoding` From `First Principles`**\n",
    "\n",
    "The very basic solution to pass the `Postional Information` to the `Self-Attention` mechanism is by adding an `Extra Dimension` to the `Word Embeddings` that represents the position of each word in the sequence.\n",
    "\n",
    "For example, consider the sentence: `\"Lion Kills Deer\"`.\n",
    "\n",
    "The `Word Embeddings` for the words `\"Lion\"`, `\"Kills\"`, and `\"Deer\"` can be represented as follows (assuming 3D embeddings for simplicity):\n",
    "\n",
    "$$\n",
    "\\text{Lion} = \\begin{bmatrix} 0.2 \\\\ 0.8 \\\\ 0.5 \\end{bmatrix}, \\quad\n",
    "\\text{Kills} = \\begin{bmatrix} 0.6 \\\\ 0.1 \\\\ 0.3 \\end{bmatrix}, \\quad\n",
    "\\text{Deer} = \\begin{bmatrix} 0.9 \\\\ 0.4 \\\\ 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we can add an `Extra Dimension` to each embedding to represent the position of each word in the sequence:\n",
    "\n",
    "$$\n",
    "\\text{Lion} = \\begin{bmatrix} 0.2 \\\\ 0.8 \\\\ 0.5 \\\\ 1 \\end{bmatrix}, \\quad\n",
    "\\text{Kills} = \\begin{bmatrix} 0.6 \\\\ 0.1 \\\\ 0.3 \\\\ 2 \\end{bmatrix}, \\quad\n",
    "\\text{Deer} = \\begin{bmatrix} 0.9 \\\\ 0.4 \\\\ 0.7 \\\\ 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, the last element in each embedding vector represents the position of the word in the sequence (1 for `\"Lion\"`, 2 for `\"Kills\"`, and 3 for `\"Deer\"`).\n",
    "\n",
    "**But, this approach has some limitations:**\n",
    "\n",
    "1. **Scalability**: As the length of the sequence increases, the positional dimension can become very large, leading to high-dimensional embeddings that are computationally expensive to process.\n",
    "\n",
    "2. **Huge Number**: The positional values can become very large for long sequences, which may lead to numerical instability during training. We know that `Neural Networks` work better with smaller values within the range of `-1 to 1` or `0 to 1`. So, if we've a sentence with `1000` words, the positional value for the last word would be `1000`, and during `Backpropagation`, this large value can lead to `Exploding Gradients`, making the training process unstable.\n",
    "\n",
    "Even if we think of normalizing these positional values by dividing them by the maximum length of the sequence, it still doesn't solve the problem of scalability and numerical instability completely.\n",
    "\n",
    "- Because, in practice, sequences can vary greatly in length, and normalizing by the maximum length may not be effective for all sequences. For sentences with different lengths, there will be no consistent way to represent positions, leading to potential confusion for the model.\n",
    "\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
