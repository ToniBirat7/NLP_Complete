{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c57237",
   "metadata": {},
   "source": [
    "## **Multi Head Attention**\n",
    "\n",
    "**Recap**\n",
    "\n",
    "First we had `Word Embeddings` which is static in nature i.e. each word has a fixed representation regardless of its context. We need a mechanism that can dynamically adjust these representations based on the surrounding words in a sentence. This is where `Self-Attention` comes into play.\n",
    "\n",
    "`Self-Attention` allows each word to attend to all other words in the sequence, enabling the model to capture contextual relationships effectively. With `Q` (`Query`), `K` (`Key`), and `V` (`Value`) vectors, we were able to compute attention scores and then generate context-aware representations for each word. So `Self-Attention` genereates `Contextual Embeddings` from `Word Embeddings`.\n",
    "\n",
    "**Promblem with Single Head Attention or Self-Attention**\n",
    "\n",
    "`Self-Attention` mechanism, was able to capture contextual relationships effectively. For example, if we've two sentences like `\"Money Bank Grows\"` and `\"River Bank Flows\"`, the word `\"Bank\"` would attend more to `\"Money\"` in the first sentence and more to `\"River\"` in the second sentence. This dynamic adjustment of attention based on context is a key strength of the `Self-Attention` mechanism.\n",
    "\n",
    "But what if we've a sentence like `\"The man saw the astronomer with the telescope\"`? Here, the word `\"with\"` can relate to either `\"astronomer\"` or `\"telescope\"`, leading to ambiguity. A single attention head might struggle to capture both interpretations effectively.\n",
    "\n",
    "This sentence can have two different interpretations:\n",
    "\n",
    "1. The man used the telescope to see the astronomer.\n",
    "\n",
    "2. The astronomer has the telescope.\n",
    "\n",
    "If we use `Self-Attention` with a single head, it might focus on `one interpretation` and miss the other. This is where `Multi-Head Attention` comes into play.\n",
    "\n",
    "To get the actual meaning of the sentence, we need to consider both interpretations simultaneously. `Multi-Head Attention` allows the model to do just that by having multiple attention heads, each focusing on different parts of the sentence.\n",
    "\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
