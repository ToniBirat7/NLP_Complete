{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad6cb36",
   "metadata": {},
   "source": [
    "This note is part of `Self Attention` of **Hand Written Notes** on **Transformer** in **NLP Complete** repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347944d3",
   "metadata": {},
   "source": [
    "## **Progess Till Now (Learning Summary)**\n",
    "\n",
    "### **Word Embedding**\n",
    "\n",
    "First we needed some way to represent `Sequential Data` like `Text` in a way that `Computers` can understand. So the most efficient way to do that is to convert each word into a `Vector` of numbers. This is called `Word Embedding`. `Embedding` is nothing but a `Dense Representation` of `Words` in a `Continuous Vector Space`. Each word is represented as a `Vector` of fixed size (like 50, 100, 300 dimensions etc.) where similar words are mapped to nearby points in that space. Some popular techniques for generating word embeddings include `Word2Vec`, `GloVe`, and `FastText`.\n",
    "\n",
    "Word Embeddings captures semantic meaning of the words. For example, the words `\"king\"` and `\"queen\"` will have similar vector representations because they share similar contexts in language.\n",
    "\n",
    "Words with similar meanings will be closer together in the vector space. For example, the vectors for `\"king\"` and `\"queen\"` will be closer to each other than to the vector for `\"car\"`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Self-Attention : First Principle**\n",
    "\n",
    "But word embeddings alone are not sufficient to capture the `Context` of a word in a sentence. For example, the word `\"bank\"` can have different meanings depending on the context (financial institution vs. river bank), such as `\"She went to the bank to deposit money\"` vs. `\"He sat by the bank of the river\"`.\n",
    "\n",
    "We need a mechanism to allow the model to focus on different parts of the input sequence when processing each word. This is where `Self-Attention` comes into play.\n",
    "\n",
    "**Self-Attention**\n",
    "\n",
    "Self-Attention is a mechanism that takes the `Raw Input Embeddings` and computes a new `Embedding` for each word by considering the entire sequence. This way, each word is represented by a new `Embedding` that considers the context provided by all other words in the sequence.\n",
    "\n",
    "To do so, Self-Attention:\n",
    "\n",
    "- First takes the `Input Embeddings` of all words in the sequence.\n",
    "\n",
    "- Then, for each word, it computes `Dot Product` with the embeddings of all other words to determine how much attention to pay to each word.\n",
    "\n",
    "- The result of the `Dot Product` is a `Scalar Value` that indicates the similarity or relevance between the words.\n",
    "\n",
    "- The result of `Dot Product` is `Scaled` i.e. `Normalized` using `Softmax Function` to convert the scores into probabilities.\n",
    "\n",
    "- The `Softmax` takes the `Dot Product` and converts into `Probabilities` that sum to 1. This helps in determining how much attention to pay to each word.\n",
    "\n",
    "- The `Probabilities` are then used to compute a `Weighted Sum` of the `Input Embeddings` (which are also derived from the input embeddings) to produce the final output embedding for each word.\n",
    "\n",
    "Below is the image that shows the basic working of `Self-Attention` mechanism.\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn.png\" alt=\"Self Attention Basic\" width=\"1400\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Contextual Embeddings**\n",
    "\n",
    "The output embeddings from the Self-Attention mechanism are called `Contextual Embeddings` because they capture the meaning of each word in the context of the entire sequence. For example, the contextual embedding for the word `\"bank\"` will be different in the two sentences mentioned earlier, reflecting its different meanings based on context.\n",
    "\n",
    "Let's we've two sentences:\n",
    "\n",
    "1. `Money Bank Grows`\n",
    "\n",
    "2. `River Bank Flows`\n",
    "\n",
    "The `Contextual Embedding` for the word `\"Bank\"` in the first sentence will be influenced more by the words `\"Money\"` and `\"Grows\"`, while in the second sentence, it will be influenced more by `\"River\"` and `\"Flows\"`. This allows the model to understand the different meanings of the same word based on its context.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc6bc3",
   "metadata": {},
   "source": [
    "## **How `Self-Attention` Computes in `Parallel`?**\n",
    "\n",
    "As we know, to calculate the `Self-Attention` for a sequence of words, we need to compute the `Dot Product` between each word's embedding and every other word's embedding in the sequence.\n",
    "\n",
    "This can be represented mathematically using `Matrix Multiplication`, which allows us to compute the `Self-Attention` scores for all words in the sequence simultaneously, rather than one at a time.\n",
    "\n",
    "So, if we've below input embeddings for a sequence of 3 words:\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn.png\" alt=\"Self Attention Matrix Multiplication\" width=\"1400\"/>\n",
    "\n",
    "We can represent these embeddings as a matrix `E`, where each row corresponds to the embedding of a word in the sequence as below:\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn_2.png\" alt=\"Self Attention Matrix Multiplication 2\" width=\"1400\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Actual Calculation of Self-Attention using Matrix Multiplication**\n",
    "\n",
    "Let's we've two sentences:\n",
    "\n",
    "1. `Money Bank Grows`\n",
    "\n",
    "2. `River Bank Flows`\n",
    "\n",
    "The input embeddings for these words are:\n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\begin{array}{l}\n",
    "\\text{(Money)}\\\\\n",
    "\\text{(Bank)}\\\\\n",
    "\\text{(Grows)}\\\\\n",
    "\\text{(River)}\\\\\n",
    "\\text{(Flows)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can represent these embeddings as a matrix `E`:\n",
    "\n",
    "$$\n",
    "E =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\begin{array}{l}\n",
    "\\text{(Money)}\\\\\n",
    "\\text{(Bank)}\\\\\n",
    "\\text{(Grows)}\\\\\n",
    "\\text{(River)}\\\\\n",
    "\\text{(Flows)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now if we multiply the matrix `E` with its transpose `E^T`, we get the `Dot Product` scores between each pair of words in the sequence:\n",
    "\n",
    "$$\n",
    "S = E \\cdot E^T\n",
    "$$\n",
    "\n",
    "Calculating the matrix multiplication:\n",
    "\n",
    "$$\n",
    "E \\cdot E^\\top =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.4 & 0.7 & 0.2 & 0.5 \\\\\n",
    "0.2 & 0.5 & 0.8 & 0.1 & 0.3 \\\\\n",
    "0.3 & 0.6 & 0.9 & 0.4 & 0.2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1\\cdot0.1+0.2\\cdot0.2+0.3\\cdot0.3 & 0.1\\cdot0.4+0.2\\cdot0.5+0.3\\cdot0.6 & 0.1\\cdot0.7+0.2\\cdot0.8+0.3\\cdot0.9 & 0.1\\cdot0.2+0.2\\cdot0.1+0.3\\cdot0.4 & 0.1\\cdot0.5+0.2\\cdot0.3+0.3\\cdot0.2 \\\\\n",
    "0.4\\cdot0.1+0.5\\cdot0.2+0.6\\cdot0.3 & 0.4\\cdot0.4+0.5\\cdot0.5+0.6\\cdot0.6 & 0.4\\cdot0.7+0.5\\cdot0.8+0.6\\cdot0.9 & 0.4\\cdot0.2+0.5\\cdot0.1+0.6\\cdot0.4 & 0.4\\cdot0.5+0.5\\cdot0.3+0.6\\cdot0.2 \\\\\n",
    "0.7\\cdot0.1+0.8\\cdot0.2+0.9\\cdot0.3 & 0.7\\cdot0.4+0.8\\cdot0.5+0.9\\cdot0.6 & 0.7\\cdot0.7+0.8\\cdot0.8+0.9\\cdot0.9 & 0.7\\cdot0.2+0.8\\cdot0.1+0.9\\cdot0.4 & 0.7\\cdot0.5+0.8\\cdot0.3+0.9\\cdot0.2 \\\\\n",
    "0.2\\cdot0.1+0.1\\cdot0.2+0.4\\cdot0.3 & 0.2\\cdot0.4+0.1\\cdot0.5+0.4\\cdot0.6 & 0.2\\cdot0.7+0.1\\cdot0.8+0.4\\cdot0.9 & 0.2\\cdot0.2+0.1\\cdot0.1+0.4\\cdot0.4 & 0.2\\cdot0.5+0.1\\cdot0.3+0.4\\cdot0.2 \\\\\n",
    "0.5\\cdot0.1+0.3\\cdot0.2+0.2\\cdot0.3 & 0.5\\cdot0.4+0.3\\cdot0.5+0.2\\cdot0.6 & 0.5\\cdot0.7+0.3\\cdot0.8+0.2\\cdot0.9 & 0.5\\cdot0.2+0.3\\cdot0.1+0.2\\cdot0.4 & 0.5\\cdot0.5+0.3\\cdot0.3+0.2\\cdot0.2\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\\\\\n",
    "\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.14 & 0.32 & 0.50 & 0.16 & 0.17 \\\\\n",
    "0.32 & 0.77 & 1.22 & 0.37 & 0.47 \\\\\n",
    "0.50 & 1.22 & 1.94 & 0.58 & 0.77 \\\\\n",
    "0.16 & 0.37 & 0.58 & 0.21 & 0.21 \\\\\n",
    "0.17 & 0.47 & 0.77 & 0.21 & 0.38\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This resulting matrix `S` contains the `Dot Product` scores between each pair of words in the sequence. Each element `S[i][j]` represents the similarity score between the `i-th` and `j-th` words based on their embeddings.\n",
    "\n",
    "Now, this matrix `S` can be further processed using `Softmax` to obtain the attention weights, which can then be used to compute the weighted sum of the input embeddings to get the final contextual embeddings for each word.\n",
    "\n",
    "**Softmax Calculation**\n",
    "\n",
    "To apply the `Softmax` function to each row of the matrix `S`, we first exponentiate each element in the row, and then normalize by dividing by the sum of the exponentiated values in that row.\n",
    "\n",
    "For example, for the first row of matrix `S`:\n",
    "\n",
    "$$\n",
    "\\text{Row 1: } [0.14, 0.32, 0.50, 0.16, 0.17]\n",
    "$$\n",
    "\n",
    "Calculating the exponentials:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e^{0.14} \\\\\n",
    "e^{0.32} \\\\\n",
    "e^{0.50} \\\\\n",
    "e^{0.16} \\\\\n",
    "e^{0.17}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1.1503 \\\\\n",
    "1.3771 \\\\\n",
    "1.6487 \\\\\n",
    "1.1735 \\\\\n",
    "1.1855\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next, we sum these exponentials:\n",
    "\n",
    "$$\n",
    "\\text{Sum} = 1.1503 + 1.3771 + 1.6487 + 1.1735 + 1.1855 = 6.5351\n",
    "$$\n",
    "\n",
    "Now, we normalize each exponentiated value by dividing by the sum:\n",
    "\n",
    "$$\n",
    "\\text{Softmax Row 1: }\n",
    "\\begin{bmatrix}\n",
    "\\frac{1.1503}{6.5351} \\\\\n",
    "\\frac{1.3771}{6.5351} \\\\\n",
    "\\frac{1.6487}{6.5351} \\\\\n",
    "\\frac{1.1735}{6.5351} \\\\\n",
    "\\frac{1.1855}{6.5351}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1761 \\\\\n",
    "0.2107 \\\\\n",
    "0.2522 \\\\\n",
    "0.1796 \\\\\n",
    "0.1814\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We would repeat this process for each row of the matrix `S` to obtain the complete attention weight matrix.\n",
    "\n",
    "So the final attention weight matrix after applying `Softmax` to each row of `S` would look like this:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0.1761 & 0.2107 & 0.2522 & 0.1796 & 0.1814 \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each row sums to 1, representing the attention weights for each word in relation to all other words in the sequence.\n",
    "\n",
    "In the `Matrix` `A` after `Softmax`, each element `A[i][j]` represents the attention weight that the `i-th` word pays to the `j-th` word in the sequence.\n",
    "\n",
    "**Computing Contextual Embeddings**\n",
    "\n",
    "Now that we have the attention weight matrix `A`, we can compute the final contextual embeddings for each word by performing a weighted sum of the input embeddings using these attention weights.\n",
    "\n",
    "Mathematically, this can be represented as:\n",
    "\n",
    "$$\n",
    "C = A \\cdot E\n",
    "$$\n",
    "\n",
    "Where `C` is the matrix of contextual embeddings, `A` is the attention weight matrix, and `E` is the input embedding matrix.\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "0.1761 & 0.2107 & 0.2522 & 0.1796 & 0.1814 \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ...\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\cdot\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute the first row:\n",
    "\n",
    "$$\n",
    "C1 = 0.1761[0.1,0.2,0.3] + 0.2107[0.4,0.5,0.6] + 0.2522[0.7,0.8,0.9] + 0.1796[0.2,0.1,0.4] + 0.1814[0.5,0.3,0.2]\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "0.40505 & 0.41471 & 0.51435 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where `C` is the matrix of contextual embeddings, `A` is the attention weight matrix, and `E` is the input embedding matrix.\n",
    "\n",
    "In the `Matrix` `C`, each row represents the new contextual embedding for each word in the sequence. For example, the first row corresponds to the contextual embedding for the word `\"Money\"`, the second row for `\"Bank\"`, and so on. These embeddings now capture the context of each word based on its relationships with all other words in the sequence.\n",
    "\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
