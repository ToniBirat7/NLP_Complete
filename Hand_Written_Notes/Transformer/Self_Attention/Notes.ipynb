{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad6cb36",
   "metadata": {},
   "source": [
    "This note is part of `Self Attention` of **Hand Written Notes** on **Transformer** in **NLP Complete** repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347944d3",
   "metadata": {},
   "source": [
    "## **Progess Till Now (Learning Summary)**\n",
    "\n",
    "### **Word Embedding**\n",
    "\n",
    "First we needed some way to represent `Sequential Data` like `Text` in a way that `Computers` can understand. So the most efficient way to do that is to convert each word into a `Vector` of numbers. This is called `Word Embedding`. `Embedding` is nothing but a `Dense Representation` of `Words` in a `Continuous Vector Space`. Each word is represented as a `Vector` of fixed size (like 50, 100, 300 dimensions etc.) where similar words are mapped to nearby points in that space. Some popular techniques for generating word embeddings include `Word2Vec`, `GloVe`, and `FastText`.\n",
    "\n",
    "Word Embeddings captures semantic meaning of the words. For example, the words `\"king\"` and `\"queen\"` will have similar vector representations because they share similar contexts in language.\n",
    "\n",
    "Words with similar meanings will be closer together in the vector space. For example, the vectors for `\"king\"` and `\"queen\"` will be closer to each other than to the vector for `\"car\"`.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f4032",
   "metadata": {},
   "source": [
    "## **Self-Attention : First Principle**\n",
    "\n",
    "Here, first we will try to understand `Self-Attention` from the basic principles. We will be discussing about `Query`, `Key` and `Value` after exploring the `Disadvantages of Word Embeddings` and `Self-Attention Mechanism`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "But word embeddings alone are not sufficient to capture the `Context` of a word in a sentence. For example, the word `\"bank\"` can have different meanings depending on the context (financial institution vs. river bank), such as `\"She went to the bank to deposit money\"` vs. `\"He sat by the bank of the river\"`.\n",
    "\n",
    "We need a mechanism to allow the model to focus on different parts of the input sequence when processing each word. This is where `Self-Attention` comes into play.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Self-Attention**\n",
    "\n",
    "Self-Attention is a mechanism that takes the `Raw Input Embeddings` and computes a new `Embedding` for each word by considering the entire sequence. This way, each word is represented by a new `Embedding` that considers the context provided by all other words in the sequence.\n",
    "\n",
    "To do so, Self-Attention:\n",
    "\n",
    "- First takes the `Input Embeddings` of all words in the sequence.\n",
    "\n",
    "- Then, for each word, it computes `Dot Product` with the embeddings of all other words to determine how much attention to pay to each word.\n",
    "\n",
    "- The result of the `Dot Product` is a `Scalar Value` that indicates the similarity or relevance between the words.\n",
    "\n",
    "- The result of `Dot Product` is `Scaled` i.e. `Normalized` using `Softmax Function` to convert the scores into probabilities.\n",
    "\n",
    "  **Why `Normalization`?**\n",
    "\n",
    "  - If we don't normalize the scores, the values can become very large or very small, leading to instability during training. Normalization helps in stabilizing the gradients and ensures that the model learns effectively.\n",
    "\n",
    "  - As `Dot Product` can produce a wide range of values, normalizing them helps in keeping the values within a manageable range, making it easier for the model to learn.\n",
    "\n",
    "  - For example, if for any word the `Dot Product` say for a word `\"bank\"` with other words produces values like `[10, 20, 30]` and for another word say `\"river\"` produces values like `[0.1, 0.2, 0.3]`, the model might struggle to learn effectively due to the large difference in scale. Normalization helps in bringing these values to a similar scale.\n",
    "\n",
    "- The `Softmax` takes the `Dot Product` and converts into `Probabilities` that sum to 1. This helps in determining how much attention to pay to each word.\n",
    "\n",
    "- The `Probabilities` are then used to compute a `Weighted Sum` of the `Input Embeddings` (which are also derived from the input embeddings) to produce the final output embedding for each word.\n",
    "\n",
    "Below is the image that shows the basic working of `Self-Attention` mechanism.\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn.png\" alt=\"Self Attention Basic\" width=\"1400\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Contextual Embeddings**\n",
    "\n",
    "The output embeddings from the Self-Attention mechanism are called `Contextual Embeddings` because they capture the meaning of each word in the context of the entire sequence. For example, the contextual embedding for the word `\"bank\"` will be different in the two sentences mentioned earlier, reflecting its different meanings based on context.\n",
    "\n",
    "Let's we've two sentences:\n",
    "\n",
    "1. `Money Bank Grows`\n",
    "\n",
    "2. `River Bank Flows`\n",
    "\n",
    "The `Contextual Embedding` for the word `\"Bank\"` in the first sentence will be influenced more by the words `\"Money\"` and `\"Grows\"`, while in the second sentence, it will be influenced more by `\"River\"` and `\"Flows\"`. This allows the model to understand the different meanings of the same word based on its context.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc6bc3",
   "metadata": {},
   "source": [
    "## **Advantages of Self-Attention**\n",
    "\n",
    "### **1. How `Self-Attention` Computes in `Parallel`?**\n",
    "\n",
    "As we know, to calculate the `Self-Attention` for a sequence of words, we need to compute the `Dot Product` between each word's embedding and every other word's embedding in the sequence.\n",
    "\n",
    "This can be represented mathematically using `Matrix Multiplication`, which allows us to compute the `Self-Attention` scores for all words in the sequence simultaneously, rather than one at a time.\n",
    "\n",
    "So, if we've below input embeddings for a sequence of 3 words:\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn.png\" alt=\"Self Attention Matrix Multiplication\" width=\"1400\"/>\n",
    "\n",
    "We can represent these embeddings as a matrix `E`, where each row corresponds to the embedding of a word in the sequence as below:\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn_2.png\" alt=\"Self Attention Matrix Multiplication 2\" width=\"1400\"/>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Actual Calculation of Self-Attention using Matrix Multiplication**\n",
    "\n",
    "Let's we've two sentences:\n",
    "\n",
    "1. `Money Bank Grows`\n",
    "\n",
    "2. `River Bank Flows`\n",
    "\n",
    "The input embeddings for these words are:\n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\begin{array}{l}\n",
    "\\text{(Money)}\\\\\n",
    "\\text{(Bank)}\\\\\n",
    "\\text{(Grows)}\\\\\n",
    "\\text{(River)}\\\\\n",
    "\\text{(Flows)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can represent these embeddings as a matrix `E`:\n",
    "\n",
    "$$\n",
    "E =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\begin{array}{l}\n",
    "\\text{(Money)}\\\\\n",
    "\\text{(Bank)}\\\\\n",
    "\\text{(Grows)}\\\\\n",
    "\\text{(River)}\\\\\n",
    "\\text{(Flows)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now if we multiply the matrix `E` with its transpose `E^T`, we get the `Dot Product` scores between each pair of words in the sequence:\n",
    "\n",
    "$$\n",
    "S = E \\cdot E^T\n",
    "$$\n",
    "\n",
    "Calculating the matrix multiplication:\n",
    "\n",
    "$$\n",
    "E \\cdot E^\\top =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.4 & 0.7 & 0.2 & 0.5 \\\\\n",
    "0.2 & 0.5 & 0.8 & 0.1 & 0.3 \\\\\n",
    "0.3 & 0.6 & 0.9 & 0.4 & 0.2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1\\cdot0.1+0.2\\cdot0.2+0.3\\cdot0.3 & 0.1\\cdot0.4+0.2\\cdot0.5+0.3\\cdot0.6 & 0.1\\cdot0.7+0.2\\cdot0.8+0.3\\cdot0.9 & 0.1\\cdot0.2+0.2\\cdot0.1+0.3\\cdot0.4 & 0.1\\cdot0.5+0.2\\cdot0.3+0.3\\cdot0.2 \\\\\n",
    "0.4\\cdot0.1+0.5\\cdot0.2+0.6\\cdot0.3 & 0.4\\cdot0.4+0.5\\cdot0.5+0.6\\cdot0.6 & 0.4\\cdot0.7+0.5\\cdot0.8+0.6\\cdot0.9 & 0.4\\cdot0.2+0.5\\cdot0.1+0.6\\cdot0.4 & 0.4\\cdot0.5+0.5\\cdot0.3+0.6\\cdot0.2 \\\\\n",
    "0.7\\cdot0.1+0.8\\cdot0.2+0.9\\cdot0.3 & 0.7\\cdot0.4+0.8\\cdot0.5+0.9\\cdot0.6 & 0.7\\cdot0.7+0.8\\cdot0.8+0.9\\cdot0.9 & 0.7\\cdot0.2+0.8\\cdot0.1+0.9\\cdot0.4 & 0.7\\cdot0.5+0.8\\cdot0.3+0.9\\cdot0.2 \\\\\n",
    "0.2\\cdot0.1+0.1\\cdot0.2+0.4\\cdot0.3 & 0.2\\cdot0.4+0.1\\cdot0.5+0.4\\cdot0.6 & 0.2\\cdot0.7+0.1\\cdot0.8+0.4\\cdot0.9 & 0.2\\cdot0.2+0.1\\cdot0.1+0.4\\cdot0.4 & 0.2\\cdot0.5+0.1\\cdot0.3+0.4\\cdot0.2 \\\\\n",
    "0.5\\cdot0.1+0.3\\cdot0.2+0.2\\cdot0.3 & 0.5\\cdot0.4+0.3\\cdot0.5+0.2\\cdot0.6 & 0.5\\cdot0.7+0.3\\cdot0.8+0.2\\cdot0.9 & 0.5\\cdot0.2+0.3\\cdot0.1+0.2\\cdot0.4 & 0.5\\cdot0.5+0.3\\cdot0.3+0.2\\cdot0.2\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\\n",
    "\\\\\n",
    "\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.14 & 0.32 & 0.50 & 0.16 & 0.17 \\\\\n",
    "0.32 & 0.77 & 1.22 & 0.37 & 0.47 \\\\\n",
    "0.50 & 1.22 & 1.94 & 0.58 & 0.77 \\\\\n",
    "0.16 & 0.37 & 0.58 & 0.21 & 0.21 \\\\\n",
    "0.17 & 0.47 & 0.77 & 0.21 & 0.38\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This resulting matrix `S` contains the `Dot Product` scores between each pair of words in the sequence. Each element `S[i][j]` represents the similarity score between the `i-th` and `j-th` words based on their embeddings.\n",
    "\n",
    "Now, this matrix `S` can be further processed using `Softmax` to obtain the attention weights, which can then be used to compute the weighted sum of the input embeddings to get the final contextual embeddings for each word.\n",
    "\n",
    "**Softmax Calculation**\n",
    "\n",
    "To apply the `Softmax` function to each row of the matrix `S`, we first exponentiate each element in the row, and then normalize by dividing by the sum of the exponentiated values in that row.\n",
    "\n",
    "For example, for the first row of matrix `S`:\n",
    "\n",
    "$$\n",
    "\\text{Row 1: } [0.14, 0.32, 0.50, 0.16, 0.17]\n",
    "$$\n",
    "\n",
    "Calculating the exponentials:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "e^{0.14} \\\\\n",
    "e^{0.32} \\\\\n",
    "e^{0.50} \\\\\n",
    "e^{0.16} \\\\\n",
    "e^{0.17}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1.1503 \\\\\n",
    "1.3771 \\\\\n",
    "1.6487 \\\\\n",
    "1.1735 \\\\\n",
    "1.1855\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next, we sum these exponentials:\n",
    "\n",
    "$$\n",
    "\\text{Sum} = 1.1503 + 1.3771 + 1.6487 + 1.1735 + 1.1855 = 6.5351\n",
    "$$\n",
    "\n",
    "Now, we normalize each exponentiated value by dividing by the sum:\n",
    "\n",
    "$$\n",
    "\\text{Softmax Row 1: }\n",
    "\\begin{bmatrix}\n",
    "\\frac{1.1503}{6.5351} \\\\\n",
    "\\frac{1.3771}{6.5351} \\\\\n",
    "\\frac{1.6487}{6.5351} \\\\\n",
    "\\frac{1.1735}{6.5351} \\\\\n",
    "\\frac{1.1855}{6.5351}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1761 \\\\\n",
    "0.2107 \\\\\n",
    "0.2522 \\\\\n",
    "0.1796 \\\\\n",
    "0.1814\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We would repeat this process for each row of the matrix `S` to obtain the complete attention weight matrix.\n",
    "\n",
    "So the final attention weight matrix after applying `Softmax` to each row of `S` would look like this:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0.1761 & 0.2107 & 0.2522 & 0.1796 & 0.1814 \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where each row sums to 1, representing the attention weights for each word in relation to all other words in the sequence.\n",
    "\n",
    "In the `Matrix` `A` after `Softmax`, each element `A[i][j]` represents the attention weight that the `i-th` word pays to the `j-th` word in the sequence.\n",
    "\n",
    "**Computing Contextual Embeddings**\n",
    "\n",
    "Now that we have the attention weight matrix `A`, we can compute the final contextual embeddings for each word by performing a weighted sum of the input embeddings using these attention weights.\n",
    "\n",
    "Mathematically, this can be represented as:\n",
    "\n",
    "$$\n",
    "C = A \\cdot E\n",
    "$$\n",
    "\n",
    "Where `C` is the matrix of contextual embeddings, `A` is the attention weight matrix, and `E` is the input embedding matrix.\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "0.1761 & 0.2107 & 0.2522 & 0.1796 & 0.1814 \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ...\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\cdot\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.2 & 0.1 & 0.4 \\\\\n",
    "0.5 & 0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute the first row:\n",
    "\n",
    "$$\n",
    "C1 = 0.1761[0.1,0.2,0.3] + 0.2107[0.4,0.5,0.6] + 0.2522[0.7,0.8,0.9] + 0.1796[0.2,0.1,0.4] + 0.1814[0.5,0.3,0.2]\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "0.40505 & 0.41471 & 0.51435 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where `C` is the matrix of contextual embeddings, `A` is the attention weight matrix, and `E` is the input embedding matrix.\n",
    "\n",
    "In the `Matrix` `C`, each row represents the new contextual embedding for each word in the sequence. For example, the first row corresponds to the contextual embedding for the word `\"Money\"`, the second row for `\"Bank\"`, and so on. These embeddings now capture the context of each word based on its relationships with all other words in the sequence.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "As we can see, by using `Matrix Multiplication`, we can efficiently compute the `Self-Attention` scores for all words in the sequence in parallel.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101508e",
   "metadata": {},
   "source": [
    "## **Disadvantages of Self-Attention**\n",
    "\n",
    "### **In the `Basic Self-Attention Mechanism`, there is no `Parameters` Involved.**\n",
    "\n",
    "**What does it mean?**\n",
    "\n",
    "So, let's start this way, whenever we always think about `Neural Networks`, we always think about `Parameters` (like `Weights` and `Biases`) that the model learns during training to make predictions. These parameters help the model to adapt and learn complex patterns from the data.\n",
    "\n",
    "And, in the basic `Self-Attention` mechanism we've discussed so far, we perform operations like `Dot Product`, `Softmax`, and `Weighted Sum` directly on the input embeddings without any learnable parameters. This means that the model doesn't have the flexibility to adjust how it computes attention based on the data it sees during training.\n",
    "\n",
    "For example, if we've build a `Model` for a specific task like `Language Translation` or `Text Summarization`, the basic `Self-Attention` mechanism won't be able to learn task-specific patterns because it lacks parameters to adapt its attention computation.\n",
    "\n",
    "If we've a sentence like `\"The cat sat on the mat\"`, the basic `Self-Attention` will compute attention scores based solely on the input embeddings of the words, without any ability to learn which words are more important for the specific task at hand.\n",
    "\n",
    "And, if we've a sentence like `\"Piece of cake\"`, the basic `Self-Attention` won't be able to learn that this phrase is an `idiom` and give it special attention, because it doesn't have parameters to learn such nuances from the training data. In this case, the model will understand the phrase literally, rather than recognizing its figurative meaning.\n",
    "\n",
    "So,\n",
    "\n",
    "If we use `Basic Self-Attention` the model will understand the phrase `Literally` or `General Meaning`, rather than the `Figurative Meaning` or `Contextual Meaning`.\n",
    "\n",
    "Therefore, if we want the model to learn `task-specific` i.e. `Sentiment` or `Translation`, we need to introduce `Learnable Parameters` into the `Self-Attention` mechanism.\n",
    "\n",
    "Without `Learnable Parameters`, the `Self-Attention` mechanism remains static i.e. independent of the `Entire Training Data` and cannot adapt to the specific patterns or nuances present in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b5a3e",
   "metadata": {},
   "source": [
    "### **What are those `Learnable Parameters`?**\n",
    "\n",
    "Once we've `Learnable Parameters` in the `Self-Attention` mechanism, we now have the flexibility to adapt how attention is computed based on the data seen during training. This allows the model to learn `task-specific` patterns and nuances from the training data, improving its performance on specific tasks like `Language Translation`, `Text Summarization`, or `Sentiment Analysis`.\n",
    "\n",
    "So, we need to introduce the `Learnable Parameters` into the `Self-Attention` mechanism. But where do we introduce those parameters?\n",
    "\n",
    "**Where do we introduce the `Learnable Parameters` in the `Self-Attention` Mechanism?**\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn_2.png\" alt=\"Self-Attention Mechanism with Learnable Parameters\" width=\"1400\"/>\n",
    "\n",
    "If we look at the above image, we can clearly see that we can only introduce the `Learnable Parameters` in the:\n",
    "\n",
    "- First `Dot Product` operation.\n",
    "\n",
    "- Last `Weighted Sum` operation.\n",
    "\n",
    "We cannot introduce `Learnable Parameters` in the `Softmax` operation because `Softmax` is a mathematical function that normalizes the scores into probabilities. It doesn't involve any learnable parameters itself.\n",
    "\n",
    "Also, if we look at the given image below:\n",
    "\n",
    "<img src=\"../../../Notes_Images/Self_Attn.png\" alt=\"Self-Attention Mechanism\" width=\"1400\"/>\n",
    "\n",
    "We can see that `Each Word` or `Word Embedding` is playing three different roles in the `Self-Attention` mechanism:\n",
    "\n",
    "Let's take `\"Money Bank Grows\"` as an example.\n",
    "\n",
    "1. **Query**: The word for which we are calculating the attention scores against all other words.\n",
    "\n",
    "- So for the word `\"Bank\"`, it acts as the `Query`.\n",
    "\n",
    "2. **Key**: The words against which we are calculating the `Attention` or `Similarity`.\n",
    "\n",
    "- So for the word `\"Bank\"` (Query), all the words `\"Money\"`, `\"Bank\"`, and `\"Grows\"` act as the `Key`.\n",
    "\n",
    "3. **Value**: The words whose embeddings we are using to compute the final output embedding after applying the attention weights. It means after calculating the `Softmax` scores, we use these scores for the `Weighted Sum` of the embeddings to get the final output embedding. The output of `Weighted Sum` gives us the new embedding for the `Query` word which is contextually aware of the other words in the sequence.\n",
    "\n",
    "- So for the words `\"Money\"`, `\"Bank\"`, and `\"Grows\"`, they also act as the `Value`.\n",
    "\n",
    "Therefore, we can introduce the `Learnable Parameters` in the form of three different `Weight Matrices` for each of these roles: `Query`, `Key`, and `Value`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164541fe",
   "metadata": {},
   "source": [
    "### **`Learnable Weight Matrices` for `Query`, `Key`, and `Value`**\n",
    "\n",
    "At first, all the `Weight Matrices` are initialized with random values. During training, these matrices are updated through backpropagation to minimize the loss function, allowing the model to learn optimal representations for `Query`, `Key`, and `Value` based on the training data.\n",
    "\n",
    "**`Query Weight Matrix (Wq)`**: This matrix is used to transform the input embeddings into `Query Vectors`. It helps the model learn how to represent the `Query` in a way that captures its importance in relation to other words.\n",
    "\n",
    "Mathematically, we can represent this transformation as:\n",
    "\n",
    "$$ Q = E \\cdot W_q $$\n",
    "\n",
    "Where, `E` is a single word embedding, `W_q` is the `Query Weight Matrix`, and `Q` is the resulting `Query Vector`.\n",
    "\n",
    "**`Key Weight Matrix (Wk)`**: This matrix is used to transform the input embeddings into `Key Vectors`. It helps the model learn how to represent the `Key` in a way that captures its relevance to the `Query`.\n",
    "\n",
    "Mathematically, we can represent this transformation as:\n",
    "\n",
    "$$ K = E \\cdot W_k $$\n",
    "\n",
    "Where, `E` is a single word embedding, `W_k` is the `Key Weight Matrix`, and `K` is the resulting `Key Vector`.\n",
    "\n",
    "**`Value Weight Matrix (Wv)`**: This matrix is used to transform the input embeddings into `Value Vectors`. It helps the model learn how to represent the `Value` in a way that captures the information needed for the final output embedding.\n",
    "\n",
    "Mathematically, we can represent this transformation as:\n",
    "\n",
    "$$ V = E \\cdot W_v $$\n",
    "\n",
    "Where, `E` is a single word embedding, `W_v` is the `Value Weight Matrix`, and `V` is the resulting `Value Vector`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Understanding `Learnable Weight Matrices` with an Example**\n",
    "\n",
    "Let's consider a sentence `\"Money Bank Grows\"` with the following input embeddings for each word:\n",
    "\n",
    "$$\n",
    "E_{Money} = [0.1, 0.2, 0.3]\n",
    "\\quad\n",
    "E_{Bank} = [0.4, 0.5, 0.6]\n",
    "\\quad\n",
    "E_{Grows} = [0.7, 0.8, 0.9]\n",
    "$$\n",
    "\n",
    "This can be represented as a matrix `E`:\n",
    "\n",
    "$$\n",
    "E =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\begin{array}{l}\n",
    "\\text{(Money)}\\\\\n",
    "\\text{(Bank)}\\\\\n",
    "\\text{(Grows)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now, let's define the `Learnable Weight Matrices` for `Query`, `Key`, and `Value` as follows:\n",
    "\n",
    "$$\n",
    "W_q =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.6 & 0.9 \\\\\n",
    "0.1 & 0.4 & 0.7 \\\\\n",
    "0.2 & 0.5 & 0.8\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "W_k =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.2 & 0.1 \\\\\n",
    "0.4 & 0.3 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "W_v =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.1 & 0.4 \\\\\n",
    "0.3 & 0.5 & 0.2 \\\\\n",
    "0.8 & 0.7 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, let's compute the `Query`, `Key`, and `Value` vectors for the word `\"Bank\"` using its input embedding `E_{Bank} = [0.4, 0.5, 0.6]`.\n",
    "\n",
    "- `Query Vector (Q)` for `\"Bank\"`:\n",
    "\n",
    "$$\n",
    "Q = E_{Bank} \\cdot W_q\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.4, 0.5, 0.6] \\cdot\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.6 & 0.9 \\\\\n",
    "0.1 & 0.4 & 0.7 \\\\\n",
    "0.2 & 0.5 & 0.8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.32, 0.59, 0.86]\n",
    "$$\n",
    "\n",
    "- `Key Vector (K)` for `\"Bank\"`:\n",
    "\n",
    "$$\n",
    "K = E_{Bank} \\cdot W_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.4, 0.5, 0.6] \\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.2 & 0.1 \\\\\n",
    "0.4 & 0.3 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- `Value Vector (V)` for `\"Bank\"`:\n",
    "\n",
    "$$\n",
    "V = E_{Bank} \\cdot W_v\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.4, 0.5, 0.6] \\cdot\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.1 & 0.4 \\\\\n",
    "0.3 & 0.5 & 0.2 \\\\\n",
    "0.8 & 0.7 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we have the `Query`, `Key`, and `Value` vectors for the word `\"Bank\"`:\n",
    "\n",
    "- `Query Vector (Q)` for `\"Bank\"`: `[0.32, 0.59, 0.86]`\n",
    "\n",
    "- `Key Vector (K)` for `\"Bank\"`: `[0.62, 0.71, 0.88]`\n",
    "\n",
    "- `Value Vector (V)` for `\"Bank\"`: `[0.74, 0.67, 0.88]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f881f90",
   "metadata": {},
   "source": [
    "### **Architecture of `Self-Attention` with `Learnable Parameters`**\n",
    "\n",
    "For each `Word Embedding` or `Token` there will be three vectors associated with it:\n",
    "\n",
    "1. **Query Vector (Q)**: This vector is used to compute the attention scores against the `Key Vectors` of all other words in the sequence.\n",
    "\n",
    "2. **Key Vector (K)**: This vector is used to compute the attention scores with the `Query Vector` of the current word.\n",
    "\n",
    "3. **Value Vector (V)**: This vector is used to compute the final output embedding after applying the attention weights.\n",
    "\n",
    "But these vectors are not directly taken from the input embeddings. Instead, they are computed by multiplying the input embeddings with three different `Learnable Weight Matrices`:\n",
    "\n",
    "- **Weight Matrix for Query (Wq)**: This matrix is used to transform the input embeddings into `Query Vectors`.\n",
    "\n",
    "- **Weight Matrix for Key (Wk)**: This matrix is used to transform the input embeddings into `Key Vectors`.\n",
    "\n",
    "- **Weight Matrix for Value (Wv)**: This matrix is used to transform the input embeddings into `Value Vectors`.\n",
    "\n",
    "So, for each word embedding `E`, we compute the `Query`, `Key`, and `Value` vectors as follows:\n",
    "\n",
    "- Query Vector: $$ Q = E \\cdot W_q $$\n",
    "\n",
    "- Key Vector: $$ K = E \\cdot W_k $$\n",
    "\n",
    "- Value Vector: $$ V = E \\cdot W_v $$\n",
    "\n",
    "Where `W_q`, `W_k`, and `W_v` are the learnable weight matrices for `Query`, `Key`, and `Value` respectively.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
