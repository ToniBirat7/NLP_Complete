{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad6cb36",
   "metadata": {},
   "source": [
    "This note is part of `Self Attention` of **Hand Written Notes** on **Transformer** in **NLP Complete** repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347944d3",
   "metadata": {},
   "source": [
    "## **Progess Till Now (Learning Summary)**\n",
    "\n",
    "### **Word Embedding**\n",
    "\n",
    "First we needed some way to represent `Sequential Data` like `Text` in a way that `Computers` can understand. So the most efficient way to do that is to convert each word into a `Vector` of numbers. This is called `Word Embedding`. `Embedding` is nothing but a `Dense Representation` of `Words` in a `Continuous Vector Space`. Each word is represented as a `Vector` of fixed size (like 50, 100, 300 dimensions etc.) where similar words are mapped to nearby points in that space. Some popular techniques for generating word embeddings include `Word2Vec`, `GloVe`, and `FastText`.\n",
    "\n",
    "Word Embeddings captures semantic meaning of the words. For example, the words `\"king\"` and `\"queen\"` will have similar vector representations because they share similar contexts in language.\n",
    "\n",
    "Words with similar meanings will be closer together in the vector space. For example, the vectors for `\"king\"` and `\"queen\"` will be closer to each other than to the vector for `\"car\"`.\n",
    "\n",
    "### **Self-Attention**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
