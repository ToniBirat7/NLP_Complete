{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1f646",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.5' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d53c1d",
   "metadata": {},
   "source": [
    "## **SetFit: Efficient Few-Shot Learning for Sentence Transformers**\n",
    "\n",
    "`SetFit` is a framework designed to enable efficient few-shot learning for [`Sentence Transformers`](https://sbert.net/)\n",
    "\n",
    "It achieves high accuracy with little labeled data - for instance, with only 8 labeled examples per class on the Customer Reviews sentiment dataset, ðŸ¤— SetFit is competitive with fine-tuning `RoBERTa` Large on the full training set of 3k examples!\n",
    "\n",
    "### **What is `Few-Shot Learning`?**\n",
    "\n",
    "Few-shot learning refers to the ability of a model to generalize and perform well on new tasks with only a small number of labeled examples. This is particularly useful in scenarios where obtaining large amounts of labeled data is impractical or expensive.\n",
    "\n",
    "### **What is `Zero-Shot Learning`?**\n",
    "\n",
    "Zero-shot learning is a related concept where a model can make predictions on tasks it has never seen before, without any labeled examples. This is typically achieved by leveraging knowledge learned from other tasks or datasets.\n",
    "\n",
    "### **What is `Sentence Transformers`?**\n",
    "\n",
    "`Sentence Transformers` or `SBERT` is a framework that allows you to easily compute dense vector representations or `Word Embeddings` for sentences and paragraphs. These embeddings can then be used for various NLP tasks such as semantic search, clustering, and classification.\n",
    "\n",
    "- `SBERT` is built on top of pre-trained transformer models like `BERT`, `RoBERTa`, and `DistilBERT`, and fine-tuned specifically for generating high-quality sentence embeddings.\n",
    "\n",
    "- We can use `SBERT` to generate `Word Embeddings` that capture the semantic meaning of sentences, making it easier to compare and analyze text data.\n",
    "\n",
    "- We can use `SBERT` to train or findtune our own `Embedding Models`, `Reranker Models`, and `Cross-Encoders` for various NLP tasks.\n",
    "\n",
    "### **What is `Cross-Encoder`?**\n",
    "\n",
    "A `Cross-Encoder` is a type of model architecture used in natural language processing tasks, particularly for tasks that involve comparing or ranking pairs of sentences or documents.\n",
    "\n",
    "- In a `Cross-Encoder`, both input sentences are processed together by the model, allowing it to capture interactions between the two inputs more effectively. This is in contrast to `Bi-Encoders`, where each sentence is processed independently, and their embeddings are compared later.\n",
    "\n",
    "For example, if we've a pair of sentences such as:\n",
    "\n",
    "1. \"The cat is sitting on the mat.\"\n",
    "\n",
    "2. \"A feline is resting on a rug.\"\n",
    "\n",
    "then, a `Cross-Encoder` would take both sentences as input simultaneously and produce a single output, such as a similarity score or classification label, based on the combined information from both sentences.\n",
    "\n",
    "We can use this `Principle` to compute similarity scores, perform ranking tasks, and classify sentence pairs in various NLP applications such as information retrieval, question answering, and duplicate detection.\n",
    "\n",
    "### **Key Features of SetFit:**\n",
    "\n",
    "- **Efficiency**: SetFit is designed to be computationally efficient, making it suitable for scenarios with limited resources.\n",
    "\n",
    "- **Few-Shot Learning**: It excels in few-shot learning scenarios, requiring only a small number of labeled examples to achieve high accuracy.\n",
    "\n",
    "- **Versatility**: SetFit can be applied to a wide range of NLP tasks, including sentiment analysis, topic classification, and more.\n",
    "\n",
    "### **How SetFit Works:**\n",
    "\n",
    "1. **Pre-trained Sentence Transformer**: `SetFit` starts with a pre-trained `Sentence Transformer` model that has been trained on a large corpus of text data.\n",
    "\n",
    "2. **Few-Shot Fine-Tuning**: The model is then fine-tuned using a small number of `labeled` examples specific to the target task. This fine-tuning process allows the model to adapt to the new task while retaining its general language understanding capabilities.\n",
    "\n",
    "3. **Evaluation**: After fine-tuning, the model is evaluated on a validation set to assess its performance on the target task.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fd919",
   "metadata": {},
   "source": [
    "## **More About SetFit**\n",
    "\n",
    "As we know `SetFit` uses `Sentence Transformers` for few-shot learning.\n",
    "\n",
    "So we can use `Sentence Transformers` model of our choice with `SetFit` framework. We can choose from a variety of pre-trained `Sentence Transformers` models available in the [`Sentence Transformers Model Hub`](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "Here, we can choose `Sentence Transformer` models for variety of `Domains` such as, `Law`, `Biomedicine`, `Finance`, `E-commerce`, `Social Media`, etc.\n",
    "\n",
    "and for variety of `Languages` such as, `English`, `German`, `French`, `Spanish`, `Chinese`, `Multilingual`, etc.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Since for now we're working on `Legal Domain`, we can choose [`Law_Model`](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/)\n",
    "\n",
    "This way we can use the `Pre-Trained Sentence Transformer` model which is specifically trained on `Legal Domain` data with `SetFit` framework for our few-shot learning tasks in legal domain.\n",
    "\n",
    "Then we can `Fine-Tune` this `Law_Model` using our small labeled dataset for specific legal tasks such as `Contract Classification`, `Legal Document Retrieval`, etc.\n",
    "\n",
    "We can load the `Pre-Trained Sentence Transformer` model using `Sentence Transformers` library and then use it with `SetFit` for our few-shot learning tasks.\n",
    "\n",
    "```python\n",
    "from setfit import SetFitModel\n",
    "\n",
    "model = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564f406",
   "metadata": {},
   "source": [
    "We will be using `Hugging Face's` `transformers` and `datasets` libraries to demonstrate how to use `SetFit` for few-shot learning.\n",
    "\n",
    "## **Getting Started with `SetFit`**\n",
    "\n",
    "### **Using a Pre-built SetFit Model (Fine-Tuning)**\n",
    "\n",
    "If we want to use a pre-built `SetFit` model from the `Hugging Face Model Hub`, we can follow these steps:\n",
    "\n",
    "1. **Install Required Libraries**:\n",
    "\n",
    "```bash\n",
    "pip install setfit transformers datasets\n",
    "```\n",
    "\n",
    "Also, if we've `CUDA` available and want to leverage GPU acceleration, ensure that the appropriate `PyTorch` version with `CUDA` support is installed.\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "```\n",
    "\n",
    "2. **Load a Pre-trained SetFit Model**:\n",
    "\n",
    "```python\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "# Load a pre-trained SetFit model\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "```\n",
    "\n",
    "Then, follow [Quick Start Guide](https://huggingface.co/docs/setfit/quickstart) to fine-tune the model on your dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
