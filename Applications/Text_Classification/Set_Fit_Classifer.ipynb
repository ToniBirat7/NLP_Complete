{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a67cb6",
   "metadata": {},
   "source": [
    "**Conda Activate**\n",
    "\n",
    "As our conda environment is stored in a external `HDD`, first we need to mount the `HDD` to access the conda environments.\n",
    "\n",
    "```bash\n",
    "# Mount the external HDD\n",
    "sudo mount -a\n",
    "```\n",
    "\n",
    "This will mount the external `HDD` where the conda environments are stored.\n",
    "\n",
    "```bash\n",
    "# Activate the base conda environment\n",
    "conda activate base\n",
    "```\n",
    "\n",
    "If not activated, then we need to run:\n",
    "\n",
    "```bash\n",
    "source ./.bashrc\n",
    "```\n",
    "\n",
    "Then we can activate the base conda environment using the above command.\n",
    "\n",
    "**NLP Conda Environment Activation**\n",
    "\n",
    "```bash\n",
    "# Run this command to activate the NLP conda environment\n",
    "conda activate nlp\n",
    "```\n",
    "\n",
    "After activating the base conda environment, you can activate the `nlp` conda environment using the above command. This environment contains all the necessary packages and dependencies for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d53c1d",
   "metadata": {},
   "source": [
    "## **SetFit: Efficient Few-Shot Learning for Sentence Transformers**\n",
    "\n",
    "[Understanding_Working_of_SetFit](https://www.youtube.com/watch?v=Pg-smN4fUy0)\n",
    "\n",
    "We know any `Machine Learning` model requires a large amount of labeled data to achieve good performance. However, in many real-world scenarios, obtaining such large labeled datasets can be challenging and expensive.\n",
    "\n",
    "`SetFit` is a framework designed to enable efficient `Few-Shot Learning` for [`Sentence Transformers`](https://sbert.net/)\n",
    "\n",
    "It achieves high accuracy with little labeled data - for instance, with only 8 labeled examples per class on the Customer Reviews sentiment dataset, ðŸ¤— `SetFit` is competitive with fine-tuning `RoBERTa` Large on the full training set of 3k examples!\n",
    "\n",
    "### **What is `Few-Shot Learning`?**\n",
    "\n",
    "Few-shot learning refers to the ability of a model to generalize and perform well on new tasks with only a small number of labeled examples. This is particularly useful in scenarios where obtaining large amounts of labeled data is impractical or expensive.\n",
    "\n",
    "### **What is `Zero-Shot Learning`?**\n",
    "\n",
    "Zero-shot learning is a related concept where a model can make predictions on tasks it has never seen before, without any labeled examples. This is typically achieved by leveraging knowledge learned from other tasks or datasets.\n",
    "\n",
    "### **What is `Sentence Transformers`?**\n",
    "\n",
    "`Sentence Transformers` or `SBERT` is a framework that allows you to easily compute dense vector representations or `Word Embeddings` for sentences and paragraphs. These embeddings can then be used for various NLP tasks such as semantic search, clustering, and classification.\n",
    "\n",
    "- `SBERT` is built on top of pre-trained transformer models like `BERT`, `RoBERTa`, and `DistilBERT`, and fine-tuned specifically for generating high-quality sentence embeddings.\n",
    "\n",
    "- We can use `SBERT` to generate `Word Embeddings` that capture the semantic meaning of sentences, making it easier to compare and analyze text data.\n",
    "\n",
    "- We can use `SBERT` to train or findtune our own `Embedding Models`, `Reranker Models`, and `Cross-Encoders` for various NLP tasks.\n",
    "\n",
    "### **What is `Cross-Encoder`?**\n",
    "\n",
    "A `Cross-Encoder` is a type of model architecture used in natural language processing tasks, particularly for tasks that involve comparing or ranking pairs of sentences or documents.\n",
    "\n",
    "- In a `Cross-Encoder`, both input sentences are processed together by the model, allowing it to capture interactions between the two inputs more effectively. This is in contrast to `Bi-Encoders`, where each sentence is processed independently, and their embeddings are compared later.\n",
    "\n",
    "For example, if we've a pair of sentences such as:\n",
    "\n",
    "1. \"The cat is sitting on the mat.\"\n",
    "\n",
    "2. \"A feline is resting on a rug.\"\n",
    "\n",
    "then, a `Cross-Encoder` would take both sentences as input simultaneously and produce a single output, such as a similarity score or classification label, based on the combined information from both sentences.\n",
    "\n",
    "We can use this `Principle` to compute similarity scores, perform ranking tasks, and classify sentence pairs in various NLP applications such as information retrieval, question answering, and duplicate detection.\n",
    "\n",
    "### **Key Features of SetFit:**\n",
    "\n",
    "- **Efficiency**: SetFit is designed to be computationally efficient, making it suitable for scenarios with limited resources.\n",
    "\n",
    "- **Few-Shot Learning**: It excels in few-shot learning scenarios, requiring only a small number of labeled examples to achieve high accuracy.\n",
    "\n",
    "- **Versatility**: SetFit can be applied to a wide range of NLP tasks, including sentiment analysis, topic classification, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7441f3a",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7a5e23",
   "metadata": {},
   "source": [
    "## **How SetFit Works:**\n",
    "\n",
    "### **Components of SetFit:**\n",
    "\n",
    "**Pre-trained Sentence Transformer**: SetFit starts with a pre-trained `Sentence Transformer` model, which provides a strong foundation for generating high-quality sentence embeddings.\n",
    "\n",
    "- This `Sentence Transformer` is used to generate the `Vector Embeddings` for the input sentences.\n",
    "\n",
    "2. **Fine-Tuning with Few Labeled Examples**: `SetFit` `Fine Tunes` the pre-trained model using a small number of labeled examples. This is done using a technique called `Contrastive Learning`, which helps the model learn to distinguish between similar and dissimilar sentence pairs.\n",
    "\n",
    "- This will create a `Vector Space` where sentences with similar meanings are closer together, while dissimilar sentences are farther apart.\n",
    "\n",
    "<img src=\"../../Notes_Images/SetFit/Contrastive_Learning.png\" alt=\"Contrastive Learning\" width=\"1200\"/>\n",
    "\n",
    "As we can see in the above image, during `Contrastive Learning`, the model is trained to minimize the distance between similar sentence pairs (positive pairs) and maximize the distance between dissimilar sentence pairs (negative pairs).\n",
    "\n",
    "Now we just need to draw a line or a hyperplane to separate the different classes in this `Vector Space`.\n",
    "\n",
    "3. **Classification Head**: After fine-tuning, a simple classification head (e.g., a `Logistic Regression` or a `Feed Forward Neural Network`) is added on top of the `Sentence Transformer` to perform the final classification task.\n",
    "\n",
    "This `Classification Head` takes the `Vector Embeddings` generated by the `Sentence Transformer` as input and produces the final predictions.\n",
    "\n",
    "### **Contrastive Training**\n",
    "\n",
    "`SetFit` employs a technique called `Contrastive Training`, where the model learns to distinguish between similar and dissimilar sentence pairs. This is achieved by creating positive and negative pairs of sentences during the fine-tuning process.\n",
    "\n",
    "For example, if we've below dataset for sentiment analysis:\n",
    "\n",
    "| Sentence                        | Label    |\n",
    "| ------------------------------- | -------- |\n",
    "| I love this product!            | Positive |\n",
    "| This is the worst service ever. | Negative |\n",
    "| The movie was fantastic!        | Positive |\n",
    "| I will never buy this again.    | Negative |\n",
    "\n",
    "`SetFit` would create positive pairs (e.g., \"I love this product!\" and \"The movie was fantastic!\") and negative pairs (e.g., \"I love this product!\" and \"This is the worst service ever.\") to train the model to recognize sentiment differences.\n",
    "\n",
    "Then, during training, the model is optimized to minimize the distance between positive pairs and maximize the distance between negative pairs in the embedding space.\n",
    "\n",
    "Finally, once the model is trained using `Contrastive Training`, a classification head is added to perform the final sentiment classification task based on the learned embeddings.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Notes**\n",
    "\n",
    "- We can `Augument` our few labeled examples using various data augmentation techniques to create more training data and improve the model's performance.\n",
    "\n",
    "- If we use other `Zero-short Classifiers` like `facebook/bart-large-mnli` or `google/flan-t5-large`, they may perform better than `SetFit` when we have extremely limited labeled data (e.g., 1-2 examples per class). However, as the number of labeled examples increases, `SetFit` tends to outperform these zero-shot classifiers due to its ability to fine-tune on the specific task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fd919",
   "metadata": {},
   "source": [
    "## **More About SetFit**\n",
    "\n",
    "As we know `SetFit` uses `Sentence Transformers` for few-shot learning.\n",
    "\n",
    "So we can use `Sentence Transformers` model of our choice with `SetFit` framework. We can choose from a variety of pre-trained `Sentence Transformers` models available in the [`Sentence Transformers Model Hub`](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "Here, we can choose `Sentence Transformer` models for variety of `Domains` such as, `Law`, `Biomedicine`, `Finance`, `E-commerce`, `Social Media`, etc.\n",
    "\n",
    "and for variety of `Languages` such as, `English`, `German`, `French`, `Spanish`, `Chinese`, `Multilingual`, etc.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Since for now we're working on `Legal Domain`, we can choose [`Law_Model`](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/)\n",
    "\n",
    "This way we can use the `Pre-Trained Sentence Transformer` model which is specifically trained on `Legal Domain` data with `SetFit` framework for our few-shot learning tasks in legal domain.\n",
    "\n",
    "Then we can `Fine-Tune` this `Law_Model` using our small labeled dataset for specific legal tasks such as `Contract Classification`, `Legal Document Retrieval`, etc.\n",
    "\n",
    "We can load the `Pre-Trained Sentence Transformer` model using `Sentence Transformers` library and then use it with `SetFit` for our few-shot learning tasks.\n",
    "\n",
    "```bash\n",
    "from setfit import SetFitModel\n",
    "\n",
    "model = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "```\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564f406",
   "metadata": {},
   "source": [
    "We will be using `Hugging Face's` `transformers` and `datasets` libraries to demonstrate how to use `SetFit` for few-shot learning.\n",
    "\n",
    "## **Getting Started with `SetFit`**\n",
    "\n",
    "### **Using a Pre-built SetFit Model (Fine-Tuning)**\n",
    "\n",
    "If we want to use a pre-built `SetFit` model from the `Hugging Face Model Hub`, we can follow these steps:\n",
    "\n",
    "1. **Install Required Libraries**:\n",
    "\n",
    "```bash\n",
    "pip install setfit transformers datasets\n",
    "```\n",
    "\n",
    "Also, if we've `CUDA` available and want to leverage GPU acceleration, ensure that the appropriate `PyTorch` version with `CUDA` support is installed.\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "```\n",
    "\n",
    "2. **Load a Pre-trained SetFit Model**:\n",
    "\n",
    "```python\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "# Load a pre-trained SetFit model\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "```\n",
    "\n",
    "Then, follow [Quick Start Guide](https://huggingface.co/docs/setfit/quickstart) to fine-tune the model on your dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
